
@article{fefferman_testing_2016,
	title = {Testing the manifold hypothesis},
	volume = {29},
	issn = {0894-0347, 1088-6834},
	url = {https://www.ams.org/jams/2016-29-04/S0894-0347-2016-00852-4/},
	doi = {10.1090/jams/852},
	language = {en},
	number = {4},
	urldate = {2022-06-24},
	journal = {Journal of the American Mathematical Society},
	author = {Fefferman, Charles and Mitter, Sanjoy and Narayanan, Hariharan},
	month = feb,
	year = {2016},
	pages = {983--1049},
	file = {Fefferman et al. - 2016 - Testing the manifold hypothesis.pdf:/home/bean/Zotero/storage/ZEVQ6MZH/Fefferman et al. - 2016 - Testing the manifold hypothesis.pdf:application/pdf},
}

@misc{razavi_generating_2019,
	title = {Generating {Diverse} {High}-{Fidelity} {Images} with {VQ}-{VAE}-2},
	url = {http://arxiv.org/abs/1906.00446},
	abstract = {We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and ﬁdelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN’s known shortcomings such as mode collapse and lack of diversity.},
	language = {en},
	urldate = {2022-06-24},
	publisher = {arXiv},
	author = {Razavi, Ali and Oord, Aaron van den and Vinyals, Oriol},
	month = jun,
	year = {2019},
	note = {arXiv:1906.00446 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Razavi et al. - 2019 - Generating Diverse High-Fidelity Images with VQ-VA.pdf:/home/bean/Zotero/storage/TJCY6E5Q/Razavi et al. - 2019 - Generating Diverse High-Fidelity Images with VQ-VA.pdf:application/pdf},
}

@incollection{cosmo_limp_2020,
	title = {{LIMP}: {Learning} {Latent} {Shape} {Representations} with {Metric} {Preservation} {Priors}},
	volume = {12348},
	shorttitle = {{LIMP}},
	url = {http://arxiv.org/abs/2003.12283},
	abstract = {In this paper, we advocate the adoption of metric preservation as a powerful prior for learning latent representations of deformable 3D shapes. Key to our construction is the introduction of a geometric distortion criterion, deﬁned directly on the decoded shapes, translating the preservation of the metric on the decoding to the formation of linear paths in the underlying latent space. Our rationale lies in the observation that training samples alone are often insuﬃcient to endow generative models with high ﬁdelity, motivating the need for large training datasets. In contrast, metric preservation provides a rigorous way to control the amount of geometric distortion incurring in the construction of the latent space, leading in turn to synthetic samples of higher quality. We further demonstrate, for the ﬁrst time, the adoption of diﬀerentiable intrinsic distances in the backpropagation of a geodesic loss. Our geometric priors are particularly relevant in the presence of scarce training data, where learning any meaningful latent structure can be especially challenging. The eﬀectiveness and potential of our generative model is showcased in applications of style transfer, content generation, and shape completion.},
	language = {en},
	urldate = {2022-06-24},
	author = {Cosmo, Luca and Norelli, Antonio and Halimi, Oshri and Kimmel, Ron and Rodolà, Emanuele},
	year = {2020},
	doi = {10.1007/978-3-030-58580-8_2},
	note = {arXiv:2003.12283 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computational Geometry, Computer Science - Graphics},
	pages = {19--35},
	file = {Cosmo et al. - 2020 - LIMP Learning Latent Shape Representations with M.pdf:/home/bean/Zotero/storage/SLGBJE9U/Cosmo et al. - 2020 - LIMP Learning Latent Shape Representations with M.pdf:application/pdf},
}

@article{zhang_manifold_2010,
	title = {Manifold {Learning} for {Visualizing} and {Analyzing} {High}-dimensional {Data}},
	issn = {1541-1672},
	url = {http://ieeexplore.ieee.org/document/5401149/},
	doi = {10.1109/MIS.2010.8},
	language = {en},
	urldate = {2022-06-25},
	journal = {IEEE Intelligent Systems},
	author = {Zhang, Junping and Huang, Hua and Wang, Jue},
	year = {2010},
	pages = {5401149},
	file = {Zhang et al. - 2010 - Manifold Learning for Visualizing and Analyzing Hi.pdf:/home/bean/Zotero/storage/TBA2GPEJ/Zhang et al. - 2010 - Manifold Learning for Visualizing and Analyzing Hi.pdf:application/pdf},
}

@article{roweis_nonlinear_2000,
	title = {Nonlinear {Dimensionality} {Reduction} by {Locally} {Linear} {Embedding}},
	volume = {290},
	url = {https://www.science.org/doi/10.1126/science.290.5500.2323},
	doi = {10.1126/science.290.5500.2323},
	number = {5500},
	urldate = {2022-06-25},
	journal = {Science},
	author = {Roweis, Sam T. and Saul, Lawrence K.},
	month = dec,
	year = {2000},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {2323--2326},
}

@article{tenenbaum_global_2000,
	title = {A global geometric framework for nonlinear dimensionality reduction},
	volume = {290},
	issn = {0036-8075},
	doi = {10.1126/science.290.5500.2319},
	abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.},
	language = {eng},
	number = {5500},
	journal = {Science (New York, N.Y.)},
	author = {Tenenbaum, J. B. and de Silva, V. and Langford, J. C.},
	month = dec,
	year = {2000},
	pmid = {11125149},
	keywords = {Algorithms, Artificial Intelligence, Face, Humans, Mathematics, Pattern Recognition, Visual, Visual Perception},
	pages = {2319--2323},
}

@misc{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	doi = {10.48550/arXiv.1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2022-06-25},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv:1301.3781 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/bean/Zotero/storage/6F5FVSR8/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf;arXiv.org Snapshot:/home/bean/Zotero/storage/WEZ5PDDU/1301.html:text/html},
}
